---
title: "HPC with R"
author: Derek Strong <br> dstrong[at]usc.edu <br> <br> Center for Advanced Research
  Computing <br> University of Southern California <br>
date: "Last updated on `r format(Sys.time(), '%Y-%m-%d')`"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      ratio: "16:10"
      titleSlideClass: ["left", "top"]
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r xaringan-themer, include = FALSE, warning = FALSE}
library(xaringanthemer)
style_mono_light(
  base_color = "#27323f",
  code_font_google   = google_font("Andale Mono"),
  code_font_size = "0.8rem",
  link_decoration = "underline",
)
```


## Outline

1. What is HPC?
2. Limitations of R
3. Profiling and benchmarking
4. Vectorizing code
5. Efficient memory use
6. Data I/O
7. Parallel programming

---

## What is HPC?

- High-performance computing
  - Relative to laptop and desktop computers
  - More processors (cluster of compute nodes)
  - More memory (shared and distributed memory)
  - High-capacity, fast-access disk storage
  - High-speed, high-bandwidth interconnects
- What HPC enables
  - **Scaling** computations to more compute resources
  - **Speedup** (faster execution times)
  - **Numerous jobs/High throughput** (running many jobs at the same time)
  - **Long-running jobs** (days or weeks)

---

## Why use R on HPC systems?

- Limitations of a laptop or workstation computer
- Space (data won't fit in memory)
- Time (takes too long to run)
- Optimized software stacks available

---

## Compute node specs

- Simplified computing model: `CPU <--> RAM <--> Disk storage`
- A compute node (individual computer) has three main components
  - Processor cores (logical CPUs)
  - Memory
  - I/O buses
- Typically HPC nodes have NUMA architecture (coupled CPU/memory domains)
- HPC clusters may have a mix of different compute nodes
  - Different types of CPUs (e.g., Intel vs. AMD)
  - Varying numbers of **cores per node** (time)
  - Varying amounts of **memory per node** (space)
  - Some nodes may also have GPUs
- On CARC systems
  - Use `nodeinfo` command to display cluster partition and node info
  - 1 logical CPU = 1 core = 1 thread (`--cpus-per-task`)
  - See more node details with: `module load usc hwloc && lstopo`

---

## Compute node layout

Example: 2 processor sockets, 8 cores each, 32 GB of memory each (NUMA architecture)

```
┌────────────────────────────────────────────────────────────────────────┐
│ Compute node (64GB of memory total)                                    │
│ ┌────────────────────────────────────────────────────────────────────┐ │
│ │ Socket/NUMA node #0 (32 GB of memory)                              │ │
│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │
│ │ │ Core #0     │  │ Core #1     │  │ Core #2     │  │ Core #3     │ │ │
│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │
│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │
│ │ │ Core #4     │  │ Core #5     │  │ Core #6     │  │ Core #7     │ │ │
│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │
│ └────────────────────────────────────────────────────────────────────┘ │
│ ┌────────────────────────────────────────────────────────────────────┐ │
│ │ Socket/NUMA node #1 (32 GB of memory)                              │ │
│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │
│ │ │ Core #0     │  │ Core #1     │  │ Core #2     │  │ Core #3     │ │ │
│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │
│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │
│ │ │ Core #4     │  │ Core #5     │  │ Core #6     │  │ Core #7     │ │ │
│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │
│ └────────────────────────────────────────────────────────────────────┘ │
└────────────────────────────────────────────────────────────────────────┘
```

---

## CPU specs

- Different CPU models have different capabilities and features
  - Instruction set architectures (e.g., x86-64, ARM64, etc.)
  - Microarchitecture designs (e.g., Intel Cascade Lake, AMD Zen 3, etc.)
  - Vector extensions (e.g., SSE4.2, AVX2, AVX-512, etc.)
  - Clock speed (aka frequency)
  - Local memory cache size
- Simply running jobs on a better CPU will reduce execution time
- Use `lscpu` to display CPU specs on a node

---

## Compiler optimizations

- R is an interpreted language, but mostly written in compiled languages (C/C++/Fortran)
- Many R functions and packages are actually interfaces to compiled programs
- Compiled programs can be optimized based on CPU architectures
- Compiler optimization flags can be used to improve performance of some R packages
  - Primarily for installing packages on Linux
  - Specify optimization flags in `~/.R/Makevars`
  - If targeting a specific CPU type, the program will only run on that CPU type
  - Examples: `data.table`, `RStan`

---

## Requesting vs. using HPC resources

- For Slurm jobs, requesting resources does not mean that the job actually uses them
- You likely need to modify your R code to make use of multiple cores or nodes
  - There are different methods to do this (implicit vs. explicit parallelism)
  - Requesting more cores/nodes does not necessarily lead to speedups
  - There is an optimal number or cores depending on the computations

---

## Limitations of R

- R is an interpreted language
  - Can be slow relative to compiled languages
  - But easier to program
- R uses a single core by default
- R stores objects in memory by default
- Maximum length of a vector is 2^31 - 1 (2,147,483,647) (32-bit integer)
- **These limitations can be overcome**

---

## General recommendations to improve performance of R code

- Code first, optimize later (if needed)
- Profile code to identify bottlenecks
- Use existing optimized solutions
- Consult package and function documentation
- Simplify when possible (do less)
- Use vectorized functions
- Modify-in-place (avoid duplicating data in memory)
- Parallelize when appropriate

---

## Profiling and benchmarking

- Aim for code that is *fast enough*
- Programmer time is more expensive than compute time
- Basic profiling workflow:

1. Profile code to understand execution time and memory use
2. Identify bottlenecks (i.e., parts of code that take the most time)
3. Try to improve performance of bottlenecks by modifying code
4. Benchmark alternative code to identify best alternative

---

## Profiling R code

- Base R: `Rprof()`, `summaryRprof()`, and `Rprofmem()`
- `profvis` for RStudio (uses `Rprof()` output file)
- `proftools` (more features, better interface)
- `profmem` for memory profiling
- `pbdPROF`, `pbdPAPI`, and `hpcvis` for MPI and low-level profiling
- Profiling output can be difficult to interpret
- Note that C/C++/Fortran code is not profiled (except with `pbdPAPI`)
- On CARC systems, download output files to view graphics locally

---

## Profiling using proftools

- Use `profileExpr()` to profile R code or script
- Profiles line-by-line and saves output
- Then use other functions to summarize output
  - `srcSummary()`
  - `flatProfile()`
  - `hotPaths()`

---

## proftools example

```{r, eval = FALSE}
library(proftools)

srcfile <- system.file("samples", "bootlmEx.R", package = "proftools")
system(paste("cat", srcfile))

pd <- profileExpr(source(srcfile))

srcSummary(pd)
```

---

## Benchmarking R code

- Base R: `system.time()`
- `bench` (more features)
- `microbenchmark` for short-running code
- `benchmarkme` for benchmarking hardware

---

## system.time() examples

```{r, eval = FALSE}
data <- data.frame(x = rexp(200000000), y = rexp(200000000))

system.time(data[data$x > 1, ])
system.time(data[which(data$x > 1), ])
system.time(subset(data, x > 1))

mat <- matrix(rexp(200000000), ncol = 800000)
data <- as.data.frame(mat)

system.time(for (i in seq_along(data)) mean(data[[i]]))
system.time(apply(data, 2, FUN = mean))
system.time(sapply(data, mean))
system.time(colMeans(data))
system.time(colMeans(mat))
```

---

## Vectorizing code

- R is designed around vectors (objects stored in column-major order)
- Vectorize code when possible to improve performance
  - Think in terms of vectors (or columns), not scalars
  - Perform operations on vectors, not individual elements
- Use vectorized functions that already exist
  - These functions are typically for loops written in C/C++/Fortran
  - Examples: arithmetic and matrix operators, `colMeans()`, `ifelse()`
- A vectorized function in R is easier to read and write

---

## Vectorizing example

```{r, eval = FALSE}
vec <- rexp(200000000)

system.time({
  test <- numeric(length(vec))
  for (i in seq_along(vec)) {
    test[i] <- vec[i] * 3
  }
})

system.time(test2 <- vec * 3)
```

---

## Efficient memory use

- If R runs out of memory
  - Within R, error message like "cannot allocate vector"
  - For Slurm jobs, error message like "oom-kill event" (out-of-memory)
  - May just need to request more memory if available
- Avoid copying data and modify-in-place when possible
- Remove objects from environment when no longer needed
- Store in simpler formats (e.g., use matrix instead of data frame when possible)
- Store data in alternative efficient formats

---

## Avoiding object duplication

- R tries not to copy objects (copy-on-modify)
- Copying slows down execution time and uses memory
- Functions that modify objects will typically copy objects before modifying
- Working with large data objects can lead to large memory use because of this
- Growing objects will duplicate objects in memory
- Pre-allocate objects when possible
- Use modify-in-place operations when possible

---

## Copy-on-modify and modify-in-place

```{r, eval = FALSE}
a <- c(1, 2, 3, 4)
tracemem(a)
b <- a
tracemem(b)
b[1] <- 5
b[3] <- 6
tracemem(b)
```

---

## When does R copy objects?

- Depends on how objects are modified and functions used
- Can be difficult to predict when copies are made
- Use `tracemem()` and memory profiling to collect data

---

## Object duplication example

```{r, eval = FALSE}
n <- 50000

# Create empty vector and grow
system.time({
  vec <- numeric(0)
  tracemem(vec)
  tr <- character(0)
  for (i in 1:n) {
    vec <- c(vec, i)
    tr <- c(tr, tracemem(vec))
  }
})
```

---

## Object duplication example (continued)

```{r, eval = FALSE}
n <- 50000

# Pre-allocate empty vector and replace values
system.time({
  vec <- numeric(n)
  tracemem(vec)
  tr <- character(n)
  for (i in 1:n) {
    vec[i] <- i
    tr[i] <- tracemem(vec)
  }
})

# In this case, just create vector directly
system.time(vec <- 1:n)
```

---

## Memory-efficient data formats

- Alternative data formats can reduce memory use
  - Smaller object size
  - Faster data I/O and data operations
  - For on-disk formats, performance limited by read/write speeds
- For data frames
  - `data.table` package for modify-in-place operations
  - `arrow` package for on-disk binary format (columnar format)
  - `fst` package for on-disk binary format (efficient reading and subsetting)
  - `disk.frame` package for on-disk binary format (multi-file fst format)
- For other data structures
  - `bigmemory` for big matrices
  - `bigstatsr` for on-disk big matrices
  - `pbdDMAT` for big matrices in MPI jobs
  - `ncdf4` or `RNetCDF` for NetCDF files (arrays)
  - `pbdNCDF4` for NetCDF files (arrays) in MPI jobs
  - `hdf5r` for HDF5 files

---

## Fast data I/O

- Base R functions for I/O are relatively slow
- For faster I/O (based on format)
  - `vroom` for tabular data
  - `readr` for tabular data (v2.0.0+ based on `vroom`)
  - `data.table` for tabular data
  - `arrow` for binary Arrow files
  - `fst` for binary fst files
- Minimize I/O when possible to improve performance
- On CARC systems, `/project`, `/scratch`, and `/scratch2` are high-performance, parallel I/O file systems

---

## Fast data processing

- Base R and `tidyverse` packages are relatively slow
- For faster data processing
  - `data.table` in general
  - `dtplyr`  for drop-in `dplyr` substitute
  - `multidplyr` for big data `dplyr` substitute (> 10M obs)
  - `bigstatsr` for big matrices (larger than memory)

---

## Parallel programming

- Simultaneous execution of different parts of a larger computation
- Data vs. task parallelism
- Tightly coupled (interdependent) vs. loosely coupled (independent) computations
- Implicit vs. explicit parallel programming
- Using one (multicore) compute node is easier than using multiple nodes
- **Scaling** computation to more processors (cores)
- Focus on **speedup** (decrease in execution time)

---

## Costs of parallelizing

- Some computations are not worth parallelizing
- Some costs to parallelizing (overhead)
  - Changing code
  - Spawning child processes
  - Copying data and environment
  - Communications
- Speedup not proportional to number of cores (Amdahl's law)
- Optimal number of cores
  - Depends on specific computations
  - Experiment to find

---

## Implicit parallelism

- Parallel programming details are abstracted away (low-effort parallelism)
- Limited to one compute node (maximize cores)
- Using optimized, multi-threaded BLAS library (not default R BLAS/LAPACK)
  - Basic Linear Algebra Subprograms
  - For example, OpenBLAS, Intel MKL, or AMD BLIS
- Using multi-threaded packages
  - Typically packages written in C/C++ and using OpenMP for multi-threading
  - If needed, simply set number of cores to use
  - Example: `data.table` is multi-threaded via OpenMP

---

## Optimized BLAS library

- On CARC systems, R modules use multi-threaded OpenBLAS
- Optimized linear algebra library used for linear algebra operations
- Will automatically use available number of cores if needed
- On CARC systems, OpenBLAS modules are multi-threaded via OpenMP
- Explicitly set number of cores to use with environment variable `OMP_NUM_THREADS`
- Or use `openblasctl::openblas_set_num_threads()`

---

## OpenBLAS example

```{r, eval = FALSE}
library(openblasctl)

mat <- matrix(rexp(4000000), ncol = 2000)

openblas_set_num_threads(1)

system.time(eigen(mat))
system.time(svd(mat))

openblas_set_num_threads(4)

system.time(eigen(mat))
system.time(svd(mat))
```

---

## Explicit parallelism

- Explicitly set up cluster of cores or nodes to parallelize over
- Single node (shared memory) vs. multiple nodes (distributed memory)
- Easier to set up with single (multicore) node
- Different types of explicit parallelism
- Which one to use depends on specific computations

---

## Conflicts with implicit and explicit parallelism

- Be careful mixing implicit and explicit parallelism
- Implicit parallel code may use more resources than intended
- Turn off implicit parallelism with `export OMP_NUM_THREADS=1`
- Or use `openblasctl::openblas_set_num_threads(1)`

---

## Some use cases for explicit parallelism

- Looping over large number of objects and applying same operations
- Running same model on many datasets
- Running many alternative models on same dataset
- Processing and analyzing data larger than memory available on single node

---

## Base R parallel

- `library(parallel)`
- Various functions for parallel computing
  - `mclapply()`, `pvec()`, `mcmapply()`
  - `makeCluster()`
  - `parApply()`, `parLapply()`, `parSapply()`
  - `mcparallel()`, `mccollect()`

---

## Using mclapply() from parallel

- Parallel version of `lapply()` using forking
  - Works on Linux or macOS
  - Will only use 1 core on Windows
- Forking is faster than making socket clusters
- Use on a single node with multiple cores
- For loosely coupled (independent) tasks (no communication needed between tasks)
- Apply same function to multiple inputs simultaneously using multiple cores

---

## mclapply() example

```{r, eval = FALSE}
# R multicore test (bootstrapping a GLM)

library(parallel)

trials <- 100000
cores <- as.numeric(Sys.getenv("SLURM_CPUS_PER_TASK"))

data <- iris[iris$Species != "setosa", c(1, 5)]
data$Species <- factor(data$Species)

model <- function(i, samp = data) {
  ind <- sample(nrow(samp), nrow(samp), replace = TRUE)
  results <- glm(samp[ind, 2] ~ samp[ind, 1], family = binomial(link = "logit"))
  coef(results)
}

system.time(lapply(1:trials, model))

system.time(mclapply(1:trials, model, mc.cores = cores))
```

---

## Job script for mclapply() example

```{sh, eval = FALSE}
#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=0
#SBATCH --time=0:20:00
#SBATCH --partition=debug
#SBATCH --account=<project_id>

module purge
module load gcc/11.2.0
module load openblas/0.3.18
module load r/4.1.2

export OMP_NUM_THREADS=1

Rscript multicore-test.R
```

---

## Using pbdLapply() from pbdMPI

- Parallel version of `lapply()` using MPI communicators
- When scaling to multiple nodes
- For loosely coupled (independent) tasks
- Apply same function to multiple inputs simultaneously using multiple cores on multiple nodes

---

## pbdLapply() example

```{r, eval = FALSE}
library(pbdMPI)

init()

trials <- 400000

data <- iris[iris$Species != "setosa", c(1, 5)]
data$Species <- factor(data$Species)

model <- function(i, samp = data) {
  ind <- sample(nrow(samp), nrow(samp), replace = TRUE)
  results <- glm(samp[ind, 2] ~ samp[ind, 1], family = binomial(link = "logit"))
  coef(results)
}

coefs <- pbdLapply(1:trials, model, pbd.mode = "spmd")

comm.print(coefs[[1]])

finalize()
```

---

## Job script for pbdLapply() example

```{sh, eval = FALSE}
#!/bin/bash

#SBATCH --nodes=2
#SBATCH --ntasks-per-node=16
#SBATCH --cpus-per-task=1
#SBATCH --mem=0
#SBATCH --time=0:10:00
#SBATCH --partition=debug
#SBATCH --account=<account_id>

module purge
module load gcc/11.2.0
module load openblas/0.3.18
module load openmpi/4.1.1
module load pmix/3.2.1
module load r/4.1.2

export OMP_NUM_THREADS=1

srun --mpi=pmix_v2 -n $SLURM_NTASKS Rscript mpi-test.R
```

---

## Creating workflows with targets

- Workflows define a set of steps (R scripts) to achieve some outcome (analysis results)
- Some of these steps are indepedent and can be run simultaneously
- Other steps are dependent on previous steps

---

## Other packages for explicit parallelism

- `foreach` for parallel for loops
- `pbdMPI` for multi-node computing with MPI
- `BiocParallel` for Bioconductor objects
- `future` / `future.batchtools` for asynchronous evaluations
- `rslurm` or `slurmR` for submitting Slurm jobs from within R
- `targets` for defining and running workflows
- Slurm job arrays or Launcher job packing can be useful too

---

## Slurm job arrays

- For submitting and managing collections of similar jobs quickly and easily
- Some use cases
  - Varying simulation or model parameters
  - Running the same models on different datasets
- Setting up a job array
  - Add `#SBATCH --array=<index>` option to job script
  - Each job task will use the same resource requests
  - Modify job or R script to use the array index
- [Slurm job array docs](https://slurm.schedmd.com/job_array.html)

---

## Job array example

```{sh, eval = FALSE}
#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=16GB
#SBATCH --time=2:00:00
#SBATCH --array=1-3

module purge
module load gcc/11.2.0
module load openblas/0.3.18
module load r/4.1.2

echo "Task ID: $SLURM_ARRAY_TASK_ID"

Rscript script.R
```

---

## Job array example (continued)

```{r, eval = FALSE}
# R script to process and model data

library(data.table)

files <- list.files("./data", full.names = TRUE)
task <- as.numeric(Sys.getenv("SLURM_ARRAY_TASK_ID"))
file <- files[task]
file

data <- fread(file)

summary(data)
```

---

## High-throughput computing

- Lots of short-running jobs (< 15 minutes)
  - Lots of serial jobs that could be run in parallel on different cores
  - Lots of parallel jobs that could be run sequentially or in parallel
- Submitting lots of jobs (> 1000) negatively impacts job scheduler
- Pack short-running jobs into one job
- Use a program like [Launcher](https://carc.usc.edu/user-information/user-guides/software-and-programming/launcher)

---

## Using GPU acceleration

- Typically not worth using GPUs (compared to multi-threaded BLAS)
- Not many well-maintained packages
- Mostly useful for machine learning packages
  - `torch`
  - `keras`
  - `tensorflow`

---

## Interfacing to a compiled language

- If your R code is still not fast enough, consider rewriting it in a compiled language
- R has a native interface for C and Fortran programs
- Use `Rcpp` family of packages for interface for C++ programs
- Which language to use depends on the data types, computations, etc.


---

## Additional resources

- [R Manuals](https://cran.r-project.org/manuals.html)
- [CRAN Task View on High-Performance and Parallel Computing with R](https://cran.r-project.org/web/views/HighPerformanceComputing.html)
- [HPCRAN](https://hpcran.org/)
- [Programming with Big Data in R](https://pbdr.org/)
- [Fastverse](https://sebkrantz.github.io/fastverse/)
- Web books
  - [The R Inferno](https://www.burns-stat.com/documents/books/the-r-inferno/)
  - [Advanced R](https://adv-r.hadley.nz/)
  - [Efficient R Programming](https://csgillespie.github.io/efficientR/)

---

## CARC support

- [Submit a support ticket](https://carc.usc.edu/user-information/ticket-submission)
- [User Forum](https://hpc-discourse.usc.edu/)
- Office Hours
  - Every Tuesday 2:30-5pm
  - Register [here](https://carc.usc.edu/news-and-events/events)
