---
title: "High-Performance Computing with R"
author: "Derek Strong <br> dstrong[at]usc.edu <br> <br> Center for Advanced Research Computing <br> University of Southern California <br>"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Outline

- What is HPC?
- Limits of R
- Profiling and benchmarking
- Vectorizing code
- Efficient memory use
- Data I/O
- Parallel programming


## What is HPC?

- High-performance computing
  - Relative to desktop/laptop computers
  - More computing power (cluster of compute nodes)
  - More memory (shared and distributed memory)
- Ability to scale computations to more compute resources
- Faster runtimes (speedup)
- Run multiple jobs at the same time
- Run jobs with long runtimes


## Hardware specs

- A compute node has multiple cores and a certain amount of memory
- HPC clusters typically have different types of compute nodes
  - Different types of CPUs (e.g., Intel vs. AMD)
  - Varying numbers of cores per node
  - Varying amounts of memory per node
  - Some nodes may also have GPUs
- On CARC systems, 1 logical CPU = 1 core = 1 thread (`--cpus-per-task`)
- Use Slurm's `sinfo` command to view cluster and node info
- [Discovery Resource Overview](https://carc.usc.edu/user-information/user-guides/hpc-basics/discovery-resources)

## Using HPC resources

- Requesting more resources for your job does not mean that it actually uses those resources
  - Maximizing cores does not necessarily lead to speedups
  - There is an optimal number or cores depending on the computations
- You likely need to modify your code to make use of more cores or multiple nodes
- There are different methods to do this (implicit vs. explicit parallelism)


## Limits of R

- R can be slow (relative to compiled languages)
- Row size is limited to 2^31 - 1 (2,147,483,647 rows)
- R uses 1 core by default
- R stores data in memory by default
- These limits can be overcome


## General recommendations to improve performance of R code

- Code first, optimize later (if needed)
- Profile code to identify bottlenecks
- Simplify when possible (do less)
- Vectorize code
- Modify-in-place (avoid duplicating data in memory)
- Parallelize when appropriate
- Use existing solutions
  - Vectorized functions (e.g., `colMeans()`, `ifelse()`, etc.)
  - Packages designed for HPC (e.g., `parallel`, `data.table`, `pbdMPI`, etc.)
- Consult package and function documentation


## Profiling and benchmarking

- Aim for code that is *fast enough*  
- Compute time is less expensive than human time
- Basic workflow:

  1. Profile code to understand the execution time and memory use of each part
  2. Identify bottlenecks (i.e., parts of code that take the most time)
  3. Try to improve performance of bottlenecks by modifying code
  4. Benchmark alternative code to identify best alternative


## Profiling R code

- Base R: `Rprof()`, `summaryRprof()`, and `Rprofmem()`
- `profvis` for RStudio (uses `Rprof()` output file)
- `proftools` (more features, better interface)
- `profmem` for memory profiling
- Can be difficult to interpret
- Note that C/C++/Fortran code is not profiled
- On CARC systems, download output files to view graphics locally


## Profiling using `proftools`

- Use `profileExpr()` to profile R script
- Profiles line-by-line and saves output
- Then use other functions to summarize output:
  - `srcSummary()`
  - `flatProfile()`
  - `hotPaths()`


## `proftools` example

```
library(proftools)

srcfile <- system.file("samples", "bootlmEx.R", package = "proftools")

pd <- profileExpr(source(srcfile))

srcSummary(pd)
```


## Benchmarking R code

- Base R: `system.time()`
- `bench` (more features)
- `microbenchmark` for short-running code
- `benchmarkme` for benchmarking hardware
- Could also use profiling tool


## `system.time()` example

```
n <- 50000

vec <- function(n) {
  x <- integer(0)
  for (i in 1:n) {
    x <- c(x, i)
  }
  x
}

system.time(vec(n))

system.time(1:n)
```


## Exercise 1

Setup:

```
mat <- matrix(rexp(200000000), ncol = 800000)

data <- as.data.frame(mat)
```

Benchmark the following functions using `system.time()`:

```
for (i in seq_along(data)) mean(data[[i]])

apply(data, 2, FUN = mean)

sapply(data, mean)

colMeans(data)

colMeans(mat)
```


## Vectorizing code

- Vectorize code when possible
  - Think about the whole object, not a single element of that object
  - Apply same operation to each element
  - Think matrix algebra (use optimized BLAS library)
- Use vectorized functions that already exist
  - These functions are typically for loops written in C/C++/Fortran
  - Easier to write and read
- For loops in R are not necessarily slow, but often not optimized or easy to write or read
  - `*apply()` family of functions are often used instead
  - See `purrr` package for a larger set of related functions
  - These functions are not really vectorized though


## Avoiding object duplication

- R tries not to copy objects (copy-on-modify)
- Copying slows down runtime and uses memory
- Functions that modify objects will typically copy object before modifying
- R can use large amounts of memory if working with large data objects
- Growing objects will duplicate objects
- Pre-allocate objects if possible


## Copy-on-modify and modify-in-place

```
a <- c(1, 2, 3, 4)
tracemem(a)
b <- a
tracemem(b)
b[[1]] <- 5
b[[3]] <- 6
tracemem(b)
z <- mean(b)
tracemem(z)
```

## When does R copy objects?

- Depends on how objects are modified and functions used
- Can be difficult to predict when copies are made
- Use `tracemem()` and memory profiling to collect data


## Object duplication example

```
n <- 100000

# Create empty vector and grow
# creates new vector with each iteration
system.time({
  vec <- numeric(0)
  for (i in 1:n) {
    vec <- c(vec, i)
  }
})
```


## Object duplication example (continued)

```
n <- 100000

# Pre-allocate empty vector and replace values
system.time({
  vec <- numeric(n)
  for (i in 1:n) {
    vec[i] <- i
  }
})
```


## Object duplication example (continued)

```
n <- 100000

# Create vector directly
system.time(vec <- 1:n)
```


## Efficient memory use

- If you run out of memory:
  - In R session, error message like "cannot allocate vector"
  - In shell, error message like "oom-kill event" (out-of-memory)
  - May just need to request more memory if available
- Avoid copying data and modify-in-place when possible
- Remove objects from environment when no longer needed
- Store in simpler formats (e.g., use matrix instead of data frame if possible)
- Store data in alternative efficient formats


## Memory-efficient data formats

- Smaller size, faster data I/O and data operations
  - Minimizes memory use
  - Limited by hardware and read/write speeds
- For data frames
  - `data.table` package for modify-in-place operations
  - `arrow` package for on-disk binary format (columnar format)
  - `fst` package for on-disk binary format (efficient reading and subsetting)
  - `disk.frame` package for on-disk binary format (multi-file fst format)
- For other data structures
  - `bigmemory` for big matrices
  - `bigstatsr` for on-disk big matrices
  - `pbdDMAT` for big matrices in MPI jobs
  - `RNetCDF` for NetCDF files (arrays)
  - `pbdNCDF4` for NetCDF files (arrays) in MPI jobs
  - `hdf5r` for HDF5 files


## Fast data I/O

- Minimize I/O if possible
- Base R functions for I/O are relatively slow
- Packages for faster I/O (based on format):
  - `readr` for tabular data
  - `vroom` for tabular data
  - `data.table` for tabular data
  - `arrow` for binary Arrow files
  - `fst` for binary fst files
- On CARC systems, `/project` and `/scratch` are high-performance, parallel I/O file systems


## Exercise 2

Setup:

```
data <- data.frame(matrix(rexp(1000000), ncol = 10000))
file <- tempfile()
```

Benchmark the following functions:

```
write.csv(data, file)

data.table::fwrite(data, file)

read.csv(file)

data.table::fread(file)
```


## Fast data processing

- Base R and Tidyverse packages are relatively slow
- Packages for faster data processing:
  - `data.table` in general
  - `dtplyr`  for `dplyr` substitute
  - `multidplyr` for big data `dplyr` substitute (> 10M obs)
  - `bigstatsr` for big matrices (larger than memory)


## Parallel programming

- Simultaneous execution of different parts of a larger computation
- Data vs. task parallelism
- Tightly coupled (interdependent) vs. loosely coupled (independent) computations
- Implicit vs. explicit parallel programming
- Using one (multicore) compute node is easier than using multiple nodes
- Key concept: speedup (decrease in runtime)


## Costs of parallelizing

- Some computations are not worth parallelizing
- Some costs to parallelizing (overhead):
  - Changing code
  - Spawning child processes
  - Copying data and environment
  - Communications
- Speedup not proportional to number of cores (Amdahl's law)
- Optimal number of cores
  - Depends on specific computations
  - Experiment to find


## Implicit parallelism

- Parallel programming details are abstracted away (low-effort parallelism)
- Limited to one compute node (maximize cores)
- Using optimized, multi-threaded BLAS library (not default R BLAS/LAPACK)
  - Basic Linear Algebra Subprograms
  - For example, OpenBLAS, Intel MKL, or AMD BLIS
- Using multi-threaded packages
  - Typically packages written in C/C++ and using OpenMP for multi-threading
  - If needed, simply set number of cores to use
  - For example, `data.table` is multi-threaded via OpenMP


## OpenBLAS

- On CARC systems, R modules use multi-threaded OpenBLAS
- Optimized linear algebra library used for linear algebra operations
- Will automatically use available number of cores if needed
- On CARC systems, OpenBLAS modules  multi-threaded via OpenMP
- Explicitly set number of cores to use with environment variable `OMP_NUM_THREADS`


## Exercise 3

Setup:

```
mat <- matrix(rexp(4000000), ncol = 2000)
```

Benchmark the following functions using 1 core and using 4 cores:

```
eigen(mat)

svd(mat)
```


## Explicit parallelism

- Explicitly set up cluster of cores or nodes to parallelize over
- Single node (shared memory) vs. multiple nodes (distributed memory)
- Easier to set up with single (multicore) node
- Different types of explicit parallelism
- Which one to use depends on specific computations
- A few packages:
  - `parallel`
  - `foreach`
  - `future` / `future.batchtools`
  - `rslurm` or `slurmR`
  - `pbdMPI`
  - `BiocParallel`
- Slurm job arrays can be useful too


## Conflicts with implicit and explicit parallelism

- Be careful mixing implicit and explicit parallelism
- Implicit parallel code may use more resources than intended
- Turn off implicit parallelism with `export OMP_NUM_THREADS=1`


## Some use cases for explicit parallelism

- Looping over large number of objects and applying same operations
- Running same model on many datasets
- Running many alternative models on same dataset
- Processing and analyzing data larger than memory available on single node


## Using `mclapply()` from `parallel`

- Parallel version of `lapply()` using forking (on Unix-like systems)
- For use on single node with multiple cores
- For loosely coupled (independent) tasks (no communication needed between tasks)
- Apply same function to multiple inputs simultaneously using multiple cores


## `mclapply()` example

```
library(parallel)

RNGkind("L'Ecuyer-CMRG")

cores <- as.numeric(Sys.getenv("SLURM_CPUS_PER_TASK")) - 1

datasets <- mclapply(1:200, function(x) data.frame(matrix(rexp(1000000), ncol = 1000)), mc.cores = cores)

model <- function(x) {
  xnames <- paste0("X", 2:1000)
  formula <- as.formula(paste("X1 ~ ", paste(xnames, collapse = "+")))
  lm(formula, x)
}

system.time(lapply(datasets, model))

system.time(mclapply(datasets, model, mc.cores = cores))
```


## Job script for `mclapply()` example

```
#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=48GB
#SBATCH --time=0:10:00

module purge
module load gcc/8.3.0
module load openblas/0.3.8
module load r/4.1.0

export OMP_NUM_THREADS=1

Rscript --vanilla multicore-test.R
```


## `lapply()` vs. `mclapply()` example output

```
system.time(lapply(datasets, model))

   user  system elapsed
 57.939   1.114  59.085
 
system.time(mclapply(datasets, model, mc.cores = cores))
 
   user  system elapsed
 13.495   6.444  21.115
```


## Other methods for explicit parallelism

- Scaling to multiple nodes and using job dependencies
- `pbdMPI` for multi-node computing
  - High-level interface to traditional MPI programming
  - Also see other [pbdR](https://pbdr.org/) packages
- `future` / `future.batchtools` for asynchronous evaluations
- `rslurm` or `slurmR` for submitting Slurm jobs from within R
- `targets` for defining and running workflows


## Using GPU acceleration

- Typically not worth using GPUs (compared to multi-threaded BLAS)
- Not many well-maintained packages
- Mostly useful for machine learning packages:
  - `torch`
  - `keras`
  - `tensorflow`


## Slurm job arrays

- For submitting and managing collections of similar jobs quickly and easily
- Some examples:
  - Varying simulation or model parameters
  - Running the same statistical models on different datasets
- [https://slurm.schedmd.com/job_array.html](https://slurm.schedmd.com/job_array.html)


## Setting up a job array

- Add `#SBATCH --array=<index>` option to job script
- Each job task will use the same resources requested
- Modify job or application script to use array index
- Multiple methods are possible


## Job array example

```
#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=16GB
#SBATCH --time=2:00:00
#SBATCH --array=1-3

module purge
module load gcc/8.3.0
module load openblas/0.3.8
module load r/4.0.0

echo "Task ID: $SLURM_ARRAY_TASK_ID"

Rscript --vanilla script.R
```


## Job array example (continued)

```
# R script to process and model data

library(data.table)

files <- list.files("./data", full.names = TRUE)
task <- as.numeric(Sys.getenv("SLURM_ARRAY_TASK_ID"))
file <- files[task]
file

data <- fread(file)

summary(data)

...
```


## Job array example (continued)

```
$ sbatch array.job
Submitted batch job 3355483

$ squeue -u ttrojan
    JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
3355483_1      main array.jo  ttrojan  R       0:05      1 d05-35
3355483_2      main array.jo  ttrojan  R       0:05      1 e16-15
3355483_3      main array.jo  ttrojan  R       0:05      1 d18-29
```


## High-throughput computing

- Lots of short-running jobs (< 15 minutes runtime)
  - Lots of serial jobs that could be run in parallel on different cores
  - Lots of parallel jobs that could be run sequentially or in parallel
- Submitting lots of jobs (> 1000) negatively impacts job scheduler
- Pack short-running jobs into one job
- Use a program like [Launcher](https://www.tacc.utexas.edu/research-development/tacc-software/the-launcher)


## Interfacing to a compiled language

- If your R code is still not fast enough, consider rewriting in and interfacing to a compiled language:
  - R has a native interface for C and Fortran programs (relatively cumbersome)
  - Use `Rcpp` / `RcppParallel` packages for C++ programs
  - Use `JuliaCall` package for Julia programs


## Additional resources

- [R Manuals](https://cran.r-project.org/manuals.html)
- [CRAN Task View on High-Performance and Parallel Computing with R](https://cran.r-project.org/web/views/HighPerformanceComputing.html)
- [HPCRAN](https://hpcran.org/)
- [Programming with Big Data in R](https://pbdr.org/)
- Web books
  - [Advanced R](https://adv-r.hadley.nz/)
  - [Efficient R Programming](https://csgillespie.github.io/efficientR/)


## Getting help

- [Submit a support ticket](https://carc.usc.edu/user-information/ticket-submission)
- [User Forum](https://hpc-discourse.usc.edu/)
- Office Hours
  - Every Tuesday 2:30-5pm (currently via Zoom)
  - Register [here](https://carc.usc.edu/news-and-events/events)
