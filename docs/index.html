<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>High Performance Computing with R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Derek Strong   dstrong[at]usc.edu     Center for Advanced Research Computing   University of Southern California  " />
    <script src="index_files/header-attrs-2.10/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: left, top, title-slide

# High Performance Computing with R
### Derek Strong <br> dstrong[at]usc.edu <br> <br> Center for Advanced Research Computing <br> University of Southern California <br>
### Last updated on 2021-10-01

---







## Outline

1. What is HPC?
2. Limitations of R
3. Profiling and benchmarking
4. Vectorizing code
5. Efficient memory use
6. Data I/O
7. Parallel programming

---

## What is HPC?

- High-performance computing system
  - Relative to laptop and desktop computers
  - More processors (cluster of compute nodes)
  - More memory (shared and distributed memory)
  - High-capacity, fast-access disk storage
  - High-speed interconnects
- What HPC enables
  - **Scaling** computations to more compute resources
  - **Speedup** (faster execution times)
  - **Parallel jobs** (running many jobs at the same time)
  - **Long-running jobs** (days or weeks)

---

## Why use R on HPC systems?

- Limitations of a laptop or desktop computer
  - Data is too big to fit in memory
  - R code takes too long to run
- Optimized software stacks available

---

## CARC HPC clusters

&lt;div style="text-align: center;"&gt;
&lt;img alt="CARC cyberinfrastructure" src="./images/cyberinfrastructure.png" width="700"/&gt;
&lt;/div&gt;

---

## Compute node specs

- A compute node (individual computer) has three main components
  - Processor cores (logical CPUs)
  - Memory
  - I/O buses
- Typically HPC nodes have NUMA architecture (coupled CPU/memory domains)
- HPC clusters may have a mix of different compute nodes
  - Different types of CPUs (e.g., Intel vs. AMD)
  - Varying numbers of cores per node
  - Varying amounts of memory per node
  - Some nodes may also have GPUs
- [Discovery Resource Overview](https://carc.usc.edu/user-information/user-guides/hpc-basics/discovery-resources)
- Use Slurm's `sinfo` command to view cluster and node info
- On CARC systems, 1 logical CPU = 1 core = 1 thread (`--cpus-per-task`)
- See node details with: `module load usc hwloc &amp;&amp; lstopo`

---

## Compute node layout

Example: 2 processor sockets, 8 cores each, 32 GB of memory each (NUMA architecture)

```
┌────────────────────────────────────────────────────────────────────────┐
│ Compute node (64GB of memory total)                                    │
│ ┌────────────────────────────────────────────────────────────────────┐ │
│ │ Socket/NUMA node #0 (32 GB of memory)                              │ │
│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │
│ │ │ Core #0     │  │ Core #1     │  │ Core #2     │  │ Core #3     │ │ │
│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │
│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │
│ │ │ Core #4     │  │ Core #5     │  │ Core #6     │  │ Core #7     │ │ │
│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │
│ └────────────────────────────────────────────────────────────────────┘ │
│ ┌────────────────────────────────────────────────────────────────────┐ │
│ │ Socket/NUMA node #1 (32 GB of memory)                              │ │
│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │
│ │ │ Core #0     │  │ Core #1     │  │ Core #2     │  │ Core #3     │ │ │
│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │
│ │ ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐ │ │
│ │ │ Core #4     │  │ Core #5     │  │ Core #6     │  │ Core #7     │ │ │
│ │ └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘ │ │
│ └────────────────────────────────────────────────────────────────────┘ │
└────────────────────────────────────────────────────────────────────────┘
```

---

## CPU specs

- Different CPU models have different capabilities and features
  - Instruction set architectures (e.g., x86-64, ARM64, etc.)
  - Microarchitecture designs (e.g., Intel Cascade Lake, AMD Zen 3, etc.)
  - Vector extensions (e.g., SSE4.2, AVX2, AVX-512, etc.)
  - Clock speed (aka frequency)
  - Local memory cache size
- See CPU specs on a node with: `lscpu`

---

## Compiler optimizations

- Compiled programs can be optimized based on CPU architectures
- R is an interpreted language, but mostly written in compiled languages (C/C++/Fortran)
- Many R functions and packages are actually interfaces to compiled programs
- Compiler flags can be used to optimize installation of R packages
  - Primarily for Linux
  - Specify optimization flags in `~/.R/Makevars` or in package source code
  - If targeting a specific CPU type, the program will only run on that CPU type

---

## Requesting vs. using HPC resources

- For Slurm jobs, requesting resources does not mean that the job actually uses them
- You likely need to modify your R code to make use of multiple cores or nodes
  - There are different methods to do this (implicit vs. explicit parallelism)
  - Requesting more cores/nodes does not necessarily lead to speedups
  - There is an optimal number or cores depending on the computations

---

## Limitations of R

- R is an interpreted language
  - Can be slow relative to compiled languages
  - But easier to program
- R uses a single core by default
- R stores objects in memory by default
- Maximum length of a vector is 2^31 - 1 (2,147,483,647)
- **These limitations can be overcome**

---

## General recommendations to improve performance of R code

- Code first, optimize later (if needed)
- Profile code to identify bottlenecks
- Use existing optimized solutions
- Consult package and function documentation
- Simplify when possible (do less)
- Use vectorized functions
- Modify-in-place (avoid duplicating data in memory)
- Parallelize when appropriate

---

## Profiling and benchmarking

- Aim for code that is *fast enough*
- Programmer time is more expensive than compute time
- Basic profiling workflow:

1. Profile code to understand execution time and memory use
2. Identify bottlenecks (i.e., parts of code that take the most time)
3. Try to improve performance of bottlenecks by modifying code
4. Benchmark alternative code to identify best alternative

---

## Profiling R code

- Base R: `Rprof()`, `summaryRprof()`, and `Rprofmem()`
- `profvis` for RStudio (uses `Rprof()` output file)
- `proftools` (more features, better interface)
- `profmem` for memory profiling
- `pbdPAPI` for low-level profiling
- Profiling output can be difficult to interpret
- Note that C/C++/Fortran code is not profiled (except with `pbdPAPI`)
- On CARC systems, download output files to view graphics locally

---

## Profiling using proftools

- Use `profileExpr()` to profile R code or script
- Profiles line-by-line and saves output
- Then use other functions to summarize output:
  - `srcSummary()`
  - `flatProfile()`
  - `hotPaths()`

---

## proftools example


```r
library(proftools)

srcfile &lt;- system.file("samples", "bootlmEx.R", package = "proftools")
system(paste("cat", srcfile))

pd &lt;- profileExpr(source(srcfile))

srcSummary(pd)
```

---

## Benchmarking R code

- Base R: `system.time()`
- `bench` (more features)
- `microbenchmark` for short-running code
- `benchmarkme` for benchmarking hardware

---

## system.time() example


```r
data &lt;- data.frame(x = rexp(200000000), y = rexp(200000000))

system.time(data[data$x &gt; 1, ])

system.time(data[which(data$x &gt; 1), ])

system.time(subset(data, x &gt; 1))
```

---

## bench example


```r
library(bench)

data &lt;- data.frame(x = rexp(200000000), y = rexp(200000000))

mark(data[data$x &gt; 1, ],
     data[which(data$x &gt; 1), ],
     subset(data, x &gt; 1))
```

---

## Exercise 1

Setup:


```r
mat &lt;- matrix(rexp(200000000), ncol = 800000)

data &lt;- as.data.frame(mat)
```

Benchmark the following functions:


```r
for (i in seq_along(data)) mean(data[[i]])

apply(data, 2, FUN = mean)

sapply(data, mean)

colMeans(data)

colMeans(mat)
```

---

## Vectorizing code

- R is designed around vectors (objects stored in column-major order)
- Vectorize code when possible to improve performance
  - Think in terms of vectors (or columns), not scalars
  - Perform operations on vectors, not individual elements
- Use vectorized functions that already exist
  - These functions are typically for loops written in C/C++/Fortran
  - Examples: arithmetic and matrix operators, `colMeans()`, `ifelse()`
- A vectorized function in R is easier to read and write

---

## Vectorizing example


```r
vec &lt;- rexp(200000000)

system.time({
  test &lt;- numeric(length(vec))
  for (i in seq_along(vec)) {
    test[i] &lt;- vec[i] * 3
  }
})

system.time(test2 &lt;- vec * 3)
```

---

## Efficient memory use

- If R runs out of memory
  - Within R, error message like "cannot allocate vector"
  - For Slurm jobs, error message like "oom-kill event" (out-of-memory)
  - May just need to request more memory if available
- Avoid copying data and modify-in-place when possible
- Remove objects from environment when no longer needed
- Store in simpler formats (e.g., use matrix instead of data frame when possible)
- Store data in alternative efficient formats

---

## Avoiding object duplication

- R tries not to copy objects (copy-on-modify)
- Copying slows down execution time and uses memory
- Functions that modify objects will typically copy objects before modifying
- Working with large data objects can lead to large memory use because of this
- Growing objects will duplicate objects in memory
- Pre-allocate objects when possible
- Use modify-in-place operations when possible

---

## Copy-on-modify and modify-in-place


```r
a &lt;- c(1, 2, 3, 4)
tracemem(a)
b &lt;- a
tracemem(b)
b[1] &lt;- 5
b[3] &lt;- 6
tracemem(b)
```

---

## When does R copy objects?

- Depends on how objects are modified and functions used
- Can be difficult to predict when copies are made
- Use `tracemem()` and memory profiling to collect data

---

## Object duplication example


```r
n &lt;- 100000

# Create empty vector and grow
system.time({
  vec &lt;- numeric(0)
  tracemem(vec)
  tr &lt;- character(0)
  for (i in 1:n) {
    vec &lt;- c(vec, i)
    tr &lt;- c(tr, tracemem(vec))
  }
})
```

---

## Object duplication example (continued)


```r
n &lt;- 100000

# Pre-allocate empty vector and replace values
system.time({
  vec &lt;- numeric(n)
  tracemem(vec)
  tr &lt;- character(n)
  for (i in 1:n) {
    vec[i] &lt;- i
    tr[i] &lt;- tracemem(vec)
  }
})

# In this case, just create vector directly
system.time(vec &lt;- 1:n)
```

---

## Memory-efficient data formats

- Alternative data formats can reduce memory use
  - Smaller object size
  - Faster data I/O and data operations
  - For on-disk formats, performance limited by read/write speeds
- For data frames
  - `data.table` package for modify-in-place operations
  - `arrow` package for on-disk binary format (columnar format)
  - `fst` package for on-disk binary format (efficient reading and subsetting)
  - `disk.frame` package for on-disk binary format (multi-file fst format)
- For other data structures
  - `bigmemory` for big matrices
  - `bigstatsr` for on-disk big matrices
  - `pbdDMAT` for big matrices in MPI jobs
  - `ncdf4` or `RNetCDF` for NetCDF files (arrays)
  - `pbdNCDF4` for NetCDF files (arrays) in MPI jobs
  - `hdf5r` for HDF5 files

---

## Fast data I/O

- Base R functions for I/O are relatively slow
- For faster I/O (based on format)
  - `vroom` for tabular data
  - `readr` for tabular data (v2.0.0+ based on `vroom`)
  - `data.table` for tabular data
  - `arrow` for binary Arrow files
  - `fst` for binary fst files
- Minimize I/O when possible to improve performance
- On CARC systems, `/project`, `/scratch`, and `/scratch2` are high-performance, parallel I/O file systems

---

## Exercise 2

Setup:


```r
data &lt;- data.frame(matrix(rexp(1000000), ncol = 10000))
file &lt;- tempfile()
library(data.table)
```

Benchmark the following functions:


```r
write.csv(data, file)

fwrite(data, file)

read.csv(file)

fread(file)
```

---

## Fast data processing

- Base R and `tidyverse` packages are relatively slow
- For faster data processing
  - `data.table` in general
  - `dtplyr`  for drop-in `dplyr` substitute
  - `multidplyr` for big data `dplyr` substitute (&gt; 10M obs)
  - `bigstatsr` for big matrices (larger than memory)

---

## Parallel programming

- Simultaneous execution of different parts of a larger computation
- Data vs. task parallelism
- Tightly coupled (interdependent) vs. loosely coupled (independent) computations
- Implicit vs. explicit parallel programming
- Using one (multicore) compute node is easier than using multiple nodes
- **Scaling** computation to more processors (cores)
- Focus on **speedup** (decrease in execution time)

---

## Costs of parallelizing

- Some computations are not worth parallelizing
- Some costs to parallelizing (overhead)
  - Changing code
  - Spawning child processes
  - Copying data and environment
  - Communications
- Speedup not proportional to number of cores (Amdahl's law)
- Optimal number of cores
  - Depends on specific computations
  - Experiment to find

---

## Implicit parallelism

- Parallel programming details are abstracted away (low-effort parallelism)
- Limited to one compute node (maximize cores)
- Using optimized, multi-threaded BLAS library (not default R BLAS/LAPACK)
  - Basic Linear Algebra Subprograms
  - For example, OpenBLAS, Intel MKL, or AMD BLIS
- Using multi-threaded packages
  - Typically packages written in C/C++ and using OpenMP for multi-threading
  - If needed, simply set number of cores to use
  - Example: `data.table` is multi-threaded via OpenMP

---

## Optimized BLAS library

- On CARC systems, R modules use multi-threaded OpenBLAS
- Optimized linear algebra library used for linear algebra operations
- Will automatically use available number of cores if needed
- On CARC systems, OpenBLAS modules are multi-threaded via OpenMP
- Explicitly set number of cores to use with environment variable `OMP_NUM_THREADS`

---

## Exercise 3

Setup:


```r
mat &lt;- matrix(rexp(4000000), ncol = 2000)
```

Benchmark the following functions using 1 core and using 4 cores:


```r
eigen(mat)

svd(mat)
```

---

## Explicit parallelism

- Explicitly set up cluster of cores or nodes to parallelize over
- Single node (shared memory) vs. multiple nodes (distributed memory)
- Easier to set up with single (multicore) node
- Different types of explicit parallelism
- Which one to use depends on specific computations

---

## Conflicts with implicit and explicit parallelism

- Be careful mixing implicit and explicit parallelism
- Implicit parallel code may use more resources than intended
- Turn off implicit parallelism with `export OMP_NUM_THREADS=1`

---

## Some use cases for explicit parallelism

- Looping over large number of objects and applying same operations
- Running same model on many datasets
- Running many alternative models on same dataset
- Processing and analyzing data larger than memory available on single node

---

## Base R parallel

- `library(parallel)`
- Various functions for parallel computing:
  - `mclapply()`, `pvec()`, `mcmapply()`
  - `makeCluster()`
  - `parApply()`, `parLapply()`, `parSapply()`
  - `mcparallel()`, `mccollect()`

---

## Using mclapply() from parallel

- Parallel version of `lapply()` using forking (Linux or macOS)
- Forking is faster than making socket clusters
- Use on a single node with multiple cores
- For loosely coupled (independent) tasks (no communication needed between tasks)
- Apply same function to multiple inputs simultaneously using multiple cores

---

## mclapply() example


```r
# R multicore test (bootstrapping a GLM)

library(parallel)

trials &lt;- 800000
cores &lt;- as.numeric(Sys.getenv("SLURM_CPUS_PER_TASK"))

model &lt;- function(i) {
  x &lt;- iris[iris$Species != "setosa", c(1, 5)]
  x$Species &lt;- factor(x$Species)
  ind &lt;- sample(100, 100, replace = TRUE)
  results &lt;- glm(x[ind, 2] ~ x[ind, 1], family = binomial(link = "logit"))
  coef(results)
}

system.time(lapply(1:trials, model))

system.time(mclapply(1:trials, model, mc.cores = cores))
```

---

## Job script for mclapply() example


```r
#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=16
#SBATCH --mem=0
#SBATCH --time=0:20:00
#SBATCH --partition=debug

module purge
module load gcc/8.3.0
module load openblas/0.3.8
module load r/4.1.0

export OMP_NUM_THREADS=1

Rscript --vanilla multicore-test.R
```

---

## Other packages for explicit parallelism

- `foreach` for parallel for loops
- `pbdMPI` for multi-node computing
  - High-level interface to traditional MPI programming
  - Also see other [pbdR](https://pbdr.org/) packages
- `BiocParallel` for Bioconductor objects
- `future` / `future.batchtools` for asynchronous evaluations
- `rslurm` or `slurmR` for submitting Slurm jobs from within R
- `targets` for defining and running workflows
- Slurm job arrays or Launcher job packing can be useful too

---

## Using GPU acceleration

- Typically not worth using GPUs (compared to multi-threaded BLAS)
- Not many well-maintained packages
- Mostly useful for machine learning packages:
  - `torch`
  - `keras`
  - `tensorflow`

---

## Slurm job arrays

- For submitting and managing collections of similar jobs quickly and easily
- Some use cases
  - Varying simulation or model parameters
  - Running the same models on different datasets
- [https://slurm.schedmd.com/job_array.html](https://slurm.schedmd.com/job_array.html)

---

## High-throughput computing

- Lots of short-running jobs (&lt; 15 minutes)
  - Lots of serial jobs that could be run in parallel on different cores
  - Lots of parallel jobs that could be run sequentially or in parallel
- Submitting lots of jobs (&gt; 1000) negatively impacts job scheduler
- Pack short-running jobs into one job
- Use a program like [Launcher](https://carc.usc.edu/user-information/user-guides/software-and-programming/launcher)

---

## Interfacing to a compiled language

- If your R code is still not fast enough, consider rewriting it in a compiled language
- R has a native interface for C and Fortran programs
- Use `Rcpp` family of packages for interface for C++ programs
- Which language to use depends on the data types, computations, etc.

---

## Additional resources

- [R Manuals](https://cran.r-project.org/manuals.html)
- [CRAN Task View on High-Performance and Parallel Computing with R](https://cran.r-project.org/web/views/HighPerformanceComputing.html)
- [HPCRAN](https://hpcran.org/)
- [Programming with Big Data in R](https://pbdr.org/)
- Web books
  - [The R Inferno](https://www.burns-stat.com/documents/books/the-r-inferno/)
  - [Advanced R](https://adv-r.hadley.nz/)
  - [Efficient R Programming](https://csgillespie.github.io/efficientR/)

---

## CARC support

- [Submit a support ticket](https://carc.usc.edu/user-information/ticket-submission)
- [User Forum](https://hpc-discourse.usc.edu/)
- Office Hours
  - Every Tuesday 2:30-5pm
  - Register [here](https://carc.usc.edu/news-and-events/events)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:10"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
